{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import misc\n",
    "import glob\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.misc import imresize\n",
    "import imageio\n",
    "import time\n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "from array import array\n",
    "\n",
    "# one hot coding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3254,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get ID\n",
    "IDList_train = genfromtxt('/usr/home/hao/work_space/FP/ITCNets/OULP-C1V2_Pack/OULP-C1V2_SubjectIDList(FormatVersion1.0)/IDList_OULP-C1V2-A-{}_Gallery.csv'.format(85)\n",
    "                        , delimiter=',', dtype=np.int32)\n",
    "IDList_train = IDList_train[:,0]\n",
    "#IDList_target = genfromtxt('/usr/home/hao/work_space/FP/ITCNets/OULP-C1V2_Pack/OULP-C1V2_SubjectIDList(FormatVersion1.0)/IDList_OULP-C1V2-A-{}_Probe.csv'.format(85)\n",
    "#                        , delimiter=',', dtype=np.int32)\n",
    "\n",
    "ID_label = IDList_train \n",
    "ID_label.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for functions\n",
    "n_class = 2254\n",
    "batch_size = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ID of the image as label\n",
    "def image_ID(img_id):\n",
    "    img_id = ''.join(map(str, img_id))\n",
    "    if len(img_id) !=7:\n",
    "        img_id = '0' *(7 - len(img_id)) + img_id\n",
    "    return(np.array(img_id))\n",
    "\n",
    "# For one subject\n",
    "\n",
    "def get_subject(features_id):\n",
    "    root_path = '/usr/home/hao/work_space/FP/ITCNets/OULP-C1V2_Pack'\n",
    "    single_subject = []\n",
    "    for ii in [1, 3, 5, 8, 10, 13, 15, 18, 20]:\n",
    "        for start_frame in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]:\n",
    "            new_feature = imread(os.path.join(root_path,'OULP-GEI-(88x128)-{}f_{}/Seq0{}'.format(ii, start_frame, 0), '{}/{}_85_GEI_{}f.png'.format(features_id, features_id, ii)))\n",
    "            new_feature = imresize(new_feature, [64, 64], interp = 'nearest')\n",
    "            single_subject.append(new_feature)\n",
    "    #single_id = (np.ones(126) * ID)       \n",
    "    return(single_subject)\n",
    "            \n",
    "            \n",
    "#person = get_subject(features_id)\n",
    "#print(np.array(person).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For n_class subjects\n",
    "def get_data(ID_label, n_class):\n",
    "    data_total = []\n",
    "    id_total = []\n",
    "    label = []\n",
    "    for ID in ID_label[0:n_class]:\n",
    "        features_id = image_ID(str(ID))\n",
    "        \n",
    "        single_id = (np.ones(126) * ID)\n",
    "        single_id = single_id.reshape(126,1)\n",
    "        #print(single_id.shape)\n",
    "        \n",
    "        #print(features_id)\n",
    "        data_unit = get_subject(features_id)\n",
    "        #data_total.append(data_unit)\n",
    "        if len(data_total) < 100:\n",
    "            data_total = data_unit\n",
    "            id_total = single_id\n",
    "        else:\n",
    "            data_total = np.vstack((data_total, data_unit))\n",
    "            id_total = np.vstack((id_total, single_id))\n",
    "    data_total = np.reshape(data_total, [data_total.shape[0], data_total.shape[1], data_total.shape[2], 1] )\n",
    "    data_total = np.array(data_total) / 255.0\n",
    "    return(data_total, id_total)\n",
    "        \n",
    "#a, b = get_data(ID_label, n_class)\n",
    "#print(np.array(a).shape)\n",
    "#print(np.array(b).shape)\n",
    "#a_ = a[0:80]\n",
    "#print(a_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_val = []\n",
    "y_train = []\n",
    "y_val = []\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "[data_total, id_total] = get_data(ID_label, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1260\n",
      "2520\n",
      "3780\n",
      "5040\n",
      "6300\n",
      "7560\n",
      "8820\n",
      "10080\n",
      "11340\n",
      "(20400, 1)\n",
      "(4800, 1)\n",
      "(20400, 64, 64, 1)\n",
      "(4800, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# To ensure train data include all subjects\n",
    "# 126 * 2254 = 284004\n",
    "for i in range(0, 126 * n_class, 126):\n",
    "    x_train_init = data_total[i:i+25]\n",
    "    x_val_init = data_total[i+25:i+40]\n",
    "    \n",
    "    y_train_init = id_total[i:i+25]\n",
    "    y_val_init = id_total[i+25:i+40]\n",
    "\n",
    "# split the rest of the data\n",
    "    ratio = 0.9\n",
    "    x_train_rest, x_val_rest, y_train_rest, y_val_rest = train_test_split(data_total[i+40 : i+126], id_total[i+40 : i+126], train_size = ratio, random_state = 123)\n",
    "    if len(x_train) < 2:\n",
    "        x_train = np.vstack((x_train_rest,x_train_init))\n",
    "        x_val = np.vstack((x_val_rest,x_val_init))\n",
    "        y_train = np.vstack((y_train_rest,y_train_init))\n",
    "        y_val = np.vstack((y_val_rest,y_val_init))\n",
    "    else:\n",
    "        x_train = np.vstack((x_train, x_train_rest,x_train_init))\n",
    "        x_val = np.vstack((x_val, x_val_rest,x_val_init))\n",
    "        y_train = np.vstack((y_train, y_train_rest,y_train_init))\n",
    "        y_val = np.vstack((y_val, y_val_rest,y_val_init))\n",
    "        if i%1260 == 0:\n",
    "            print(i)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "\n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/Classifier/data/GEI64x64_For_train_x', 'wb') as f:\n",
    "    np.save(f, x_train)\n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/Classifier/data/GEI64x64_For_train_y', 'wb') as f:\n",
    "    np.save(f, x_val)\n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/Classifier/data/GEI64x64_For_val_x', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "with open('/usr/home/hao/work_space/FP/ITCNets/Classifier/data/GEI64x64_For_val_y', 'wb') as f:\n",
    "    np.save(f, y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Build GEINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.6\n",
    "# Input and label\n",
    "inputs_ = tf.placeholder(tf.float32, [None, 64, 64, 1], name='inputs_')\n",
    "target_ = tf.placeholder(tf.float32, [None, n_class], name='target_')\n",
    "\n",
    "# Build layers of the GEINet\n",
    "\n",
    "# 1. [64,64,1] to [64,64,18]\n",
    "conv1 = tf.layers.conv2d(inputs_,filters = 18,kernel_size =[7,7], strides = 1, padding = 'same', activation=tf.nn.relu,\n",
    "                        kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "# pooling [64,64,18] to [32,32,9]\n",
    "conv1 = tf.layers.max_pooling2d(conv1, (2,2), strides = 2, padding='same')\n",
    "\n",
    "# Add normalization layer\n",
    "conv1 = tf.layers.batch_normalization(conv1, training=True)\n",
    "###############################################################################\n",
    "# 2. [32,32,9] to [32,32,45]\n",
    "conv2 = tf.layers.conv2d(conv1, 45, kernel_size =[5,5], strides = 1, padding = 'same', activation=tf.nn.relu,\n",
    "                        kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "# pooling [32,32,45] to [16,16,15]\n",
    "conv2 = tf.layers.max_pooling2d(conv2, (3,3), strides = 2, padding='same')\n",
    "\n",
    "# Add normalization layer\n",
    "conv2 = tf.layers.batch_normalization(conv2, training=True)\n",
    "###############################################################################\n",
    "# Rebuild the output\n",
    "shape = np.prod(conv2.get_shape().as_list()[1:])\n",
    "conv2 = tf.reshape(conv2, [-1, shape])\n",
    "\n",
    "# 1. FC [16,16,15] to [1*1024]\n",
    "fc1 = tf.contrib.layers.fully_connected(conv2, 1024, activation_fn=tf.nn.relu)\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# 2. FC [1*1024] to [1*512]\n",
    "fc2  = tf.contrib.layers.fully_connected(fc1, 512, activation_fn=tf.nn.relu)\n",
    "\n",
    "# Logits [1*512] to [1*n_class]\n",
    "logits_ = tf.contrib.layers.fully_connected(fc2, n_class, activation_fn=None)\n",
    "logits_ = tf.identity(logits_, name='logits_')\n",
    "\n",
    "# cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_, labels=target_))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits_, 1), tf.argmax(target_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1-th epochs start:\n",
      "Batch  1, Train Loss 5.1095, Train Accuracy %100.0000\n",
      "Validation Accuracy %100.0000\n",
      "Batch 21, Train Loss 0.0000, Train Accuracy %100.0000\n",
      "Validation Accuracy %98.7500\n",
      "Batch 41, Train Loss 0.0000, Train Accuracy %100.0000\n",
      "Validation Accuracy %98.7500\n",
      "Batch 61, Train Loss 0.0001, Train Accuracy %100.0000\n",
      "Validation Accuracy %100.0000\n",
      "Batch 81, Train Loss 0.0000, Train Accuracy %98.7500\n",
      "Validation Accuracy %100.0000\n",
      "Batch 101, Train Loss 0.2754, Train Accuracy %98.7500\n",
      "Validation Accuracy %100.0000\n",
      "Batch 121, Train Loss 0.0000, Train Accuracy %100.0000\n",
      "Validation Accuracy %100.0000\n",
      "Batch 141, Train Loss 0.0000, Train Accuracy %100.0000\n",
      "Validation Accuracy %100.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-41cc7edb6fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             train_acc  = sess.run(accuracy, \n\u001b[1;32m     38\u001b[0m                                   feed_dict = {inputs_: train_, \n\u001b[0;32m---> 39\u001b[0;31m                                             target_: train_id})\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# For validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/home/hao/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/home/hao/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/home/hao/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/home/hao/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/home/hao/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Basic parameters\n",
    "data_shape = x_train.shape[0]\n",
    "val_shape = x_val.shape[0]\n",
    "epochs = 800\n",
    "\n",
    "save_model_path = '/usr/home/hao/work_space/FP/ITCNets/Classifier'\n",
    "#count = 0\n",
    "\n",
    "lb = LabelBinarizer().fit(np.array(range(n_class)))\n",
    "train_ = []\n",
    "\n",
    "train_id = []\n",
    "val_ = []\n",
    "val_id = []\n",
    "iteration  = 0\n",
    "val_batch_size = 80\n",
    "n_batches = data_shape // batch_size\n",
    "\n",
    "val_batches = val_shape // val_batch_size\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        print(\"The {}-th epochs start:\".format(epoch +1 ))\n",
    "        for ii in range(data_shape//batch_size -1):\n",
    "            train_ = x_train[ii : ii + batch_size]\n",
    "            train_id = y_train[ii : ii + batch_size]\n",
    "      \n",
    "                \n",
    "            train_id = lb.transform(train_id)\n",
    "           \n",
    "      \n",
    "            train_loss, _ = sess.run([cost, optimizer], \n",
    "                                     feed_dict = {inputs_: train_, \n",
    "                                                  target_: train_id})\n",
    "            train_acc  = sess.run(accuracy, \n",
    "                                  feed_dict = {inputs_: train_, \n",
    "                                            target_: train_id})\n",
    "            # For validation\n",
    "            \n",
    "            if(iteration%20 == 0):\n",
    "             \n",
    "                print('Batch {:>2}, Train Loss {:.4f}, Train Accuracy %{:.4f}'.format(iteration + 1,\n",
    "                                                        train_loss,\n",
    "                                                        train_acc))\n",
    "            #if iteration == 20 or iteration%100 ==0:\n",
    "                #validation_loss = []\n",
    "                validation_acc = []\n",
    "                \n",
    "                \n",
    "                for jj in range(val_shape // val_batch_size -1):\n",
    "                    val_ = x_val[jj : jj + val_batch_size]\n",
    "                    val_id = y_val[jj : jj + val_batch_size]\n",
    "                    \n",
    "                    val_id = lb.transform(val_id)\n",
    "                    val_loss, _ = sess.run([cost, optimizer], \n",
    "                                     feed_dict = {inputs_: val_, \n",
    "                                                  target_: val_id})\n",
    "                    val_acc = sess.run(accuracy, \n",
    "                                       feed_dict = {inputs_: val_, \n",
    "                                                    target_: val_id})\n",
    "                    validation_acc.append(val_acc)\n",
    "                mean_val_acc = np.mean(np.array(validation_acc))\n",
    "                #print(\"Validation accuracy: %{:.3f},\".format(mean_val_acc))\n",
    "                print('Validation Accuracy %{:.4f}'.format(val_acc))\n",
    "            iteration += 1                   \n",
    "           \n",
    "            \n",
    "        #count += 1\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
